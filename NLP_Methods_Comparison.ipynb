{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Models and Vectorization Strategies for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Objective \n",
    "\n",
    "The objective of this project is to preprocess and analyze text data using various techniques to improve classification performance. We will apply text preprocessing methods, including stemming and lemmatizing, and utilize CountVectorizer and TfidfVectorizer for feature extraction. We will then evaluate the performance of different classification algorithms, namely Logistic Regression, Decision Tree, and Multinomial Naive Bayes, by comparing their accuracy and computational efficiency. The final goal is to identify the best-performing model and present the results, including the best parameters and scores, in a clear and comprehensive format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Outline\n",
    "\n",
    "1. Import required libraries\n",
    "2. Connect to the data source and explore it \n",
    "3. Text preprocessing:Stemming, Lemmatizing, CountVectorizer and TfidifVectorizer \n",
    "4. Classification: LogisticRegression, DecisionTreeClassifier, and MultinomialNB\n",
    "5. Performance analysis:accuracy and speed\n",
    "6. Conclusion summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The dataset below is from [kaggle]() and contains a dataset named the \"ColBert Dataset\" created for this [paper](https://arxiv.org/pdf/2004.12765.pdf).  The project will use the text column to classify whether or not the text was humorous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\agnek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agnek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\agnek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time  # Ensure to import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download the 'omw-1.4' resource\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to the data source\n",
    "df = pd.read_csv(r'C:\\Users\\agnek\\OneDrive\\Documents\\Data\\dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without its shell? d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 reasons the 2016 election feels so personal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pasco police shot mexican migrant from behind,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
       "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
       "2  What do you call a turtle without its shell? d...   True\n",
       "3      5 reasons the 2016 election feels so personal  False\n",
       "4  Pasco police shot mexican migrant from behind,...  False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Review data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stemmed_tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def lemmatized_tokenizer(text):\n",
    "    tokens = text.split()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features and target variable\n",
    "X = df['text']\n",
    "y = df['humor'] \n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorizers with stop words and max features\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using CountVectorizer with Stemming:\n",
      "\n",
      "\n",
      "Training LogisticRegression:\n",
      "\n",
      "Accuracy: 0.8878\n",
      "Training Time: 1.80 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89     29909\n",
      "           1       0.88      0.89      0.89     30091\n",
      "\n",
      "    accuracy                           0.89     60000\n",
      "   macro avg       0.89      0.89      0.89     60000\n",
      "weighted avg       0.89      0.89      0.89     60000\n",
      "\n",
      "\n",
      "Training DecisionTreeClassifier:\n",
      "\n",
      "Accuracy: 0.81685\n",
      "Training Time: 36.64 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81     29909\n",
      "           1       0.81      0.83      0.82     30091\n",
      "\n",
      "    accuracy                           0.82     60000\n",
      "   macro avg       0.82      0.82      0.82     60000\n",
      "weighted avg       0.82      0.82      0.82     60000\n",
      "\n",
      "\n",
      "Training MultinomialNB:\n",
      "\n",
      "Accuracy: 0.8793833333333333\n",
      "Training Time: 0.02 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     29909\n",
      "           1       0.88      0.88      0.88     30091\n",
      "\n",
      "    accuracy                           0.88     60000\n",
      "   macro avg       0.88      0.88      0.88     60000\n",
      "weighted avg       0.88      0.88      0.88     60000\n",
      "\n",
      "\n",
      "Using CountVectorizer with Lemmatizing:\n",
      "\n",
      "\n",
      "Training LogisticRegression:\n",
      "\n",
      "Accuracy: 0.8829333333333333\n",
      "Training Time: 1.85 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     29909\n",
      "           1       0.88      0.89      0.88     30091\n",
      "\n",
      "    accuracy                           0.88     60000\n",
      "   macro avg       0.88      0.88      0.88     60000\n",
      "weighted avg       0.88      0.88      0.88     60000\n",
      "\n",
      "\n",
      "Training DecisionTreeClassifier:\n",
      "\n",
      "Accuracy: 0.8101666666666667\n",
      "Training Time: 35.62 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.81     29909\n",
      "           1       0.81      0.82      0.81     30091\n",
      "\n",
      "    accuracy                           0.81     60000\n",
      "   macro avg       0.81      0.81      0.81     60000\n",
      "weighted avg       0.81      0.81      0.81     60000\n",
      "\n",
      "\n",
      "Training MultinomialNB:\n",
      "\n",
      "Accuracy: 0.87595\n",
      "Training Time: 0.02 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88     29909\n",
      "           1       0.87      0.88      0.88     30091\n",
      "\n",
      "    accuracy                           0.88     60000\n",
      "   macro avg       0.88      0.88      0.88     60000\n",
      "weighted avg       0.88      0.88      0.88     60000\n",
      "\n",
      "\n",
      "Using TfidfVectorizer with Stemming:\n",
      "\n",
      "\n",
      "Training LogisticRegression:\n",
      "\n",
      "Accuracy: 0.8877333333333334\n",
      "Training Time: 1.66 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89     29909\n",
      "           1       0.89      0.89      0.89     30091\n",
      "\n",
      "    accuracy                           0.89     60000\n",
      "   macro avg       0.89      0.89      0.89     60000\n",
      "weighted avg       0.89      0.89      0.89     60000\n",
      "\n",
      "\n",
      "Training DecisionTreeClassifier:\n",
      "\n",
      "Accuracy: 0.8242333333333334\n",
      "Training Time: 41.93 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.82     29909\n",
      "           1       0.82      0.83      0.83     30091\n",
      "\n",
      "    accuracy                           0.82     60000\n",
      "   macro avg       0.82      0.82      0.82     60000\n",
      "weighted avg       0.82      0.82      0.82     60000\n",
      "\n",
      "\n",
      "Training MultinomialNB:\n",
      "\n",
      "Accuracy: 0.8788\n",
      "Training Time: 0.02 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     29909\n",
      "           1       0.88      0.88      0.88     30091\n",
      "\n",
      "    accuracy                           0.88     60000\n",
      "   macro avg       0.88      0.88      0.88     60000\n",
      "weighted avg       0.88      0.88      0.88     60000\n",
      "\n",
      "\n",
      "Using TfidfVectorizer with Lemmatizing:\n",
      "\n",
      "\n",
      "Training LogisticRegression:\n",
      "\n",
      "Accuracy: 0.8828\n",
      "Training Time: 1.15 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     29909\n",
      "           1       0.88      0.88      0.88     30091\n",
      "\n",
      "    accuracy                           0.88     60000\n",
      "   macro avg       0.88      0.88      0.88     60000\n",
      "weighted avg       0.88      0.88      0.88     60000\n",
      "\n",
      "\n",
      "Training DecisionTreeClassifier:\n",
      "\n",
      "Accuracy: 0.8213666666666667\n",
      "Training Time: 42.12 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82     29909\n",
      "           1       0.82      0.83      0.82     30091\n",
      "\n",
      "    accuracy                           0.82     60000\n",
      "   macro avg       0.82      0.82      0.82     60000\n",
      "weighted avg       0.82      0.82      0.82     60000\n",
      "\n",
      "\n",
      "Training MultinomialNB:\n",
      "\n",
      "Accuracy: 0.8729\n",
      "Training Time: 0.02 seconds\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87     29909\n",
      "           1       0.87      0.87      0.87     30091\n",
      "\n",
      "    accuracy                           0.87     60000\n",
      "   macro avg       0.87      0.87      0.87     60000\n",
      "weighted avg       0.87      0.87      0.87     60000\n",
      "\n",
      "\n",
      "Best Results by Vectorizer, Stemming/Lemmatizing, and Model:\n",
      "         Vectorizer Stemming/Lemmatizing                   Model  Accuracy  \\\n",
      "4   CountVectorizer          Lemmatizing  DecisionTreeClassifier  0.810167   \n",
      "3   CountVectorizer          Lemmatizing      LogisticRegression  0.882933   \n",
      "5   CountVectorizer          Lemmatizing           MultinomialNB  0.875950   \n",
      "1   CountVectorizer             Stemming  DecisionTreeClassifier  0.816850   \n",
      "0   CountVectorizer             Stemming      LogisticRegression  0.887800   \n",
      "2   CountVectorizer             Stemming           MultinomialNB  0.879383   \n",
      "10  TfidfVectorizer          Lemmatizing  DecisionTreeClassifier  0.821367   \n",
      "9   TfidfVectorizer          Lemmatizing      LogisticRegression  0.882800   \n",
      "11  TfidfVectorizer          Lemmatizing           MultinomialNB  0.872900   \n",
      "7   TfidfVectorizer             Stemming  DecisionTreeClassifier  0.824233   \n",
      "6   TfidfVectorizer             Stemming      LogisticRegression  0.887733   \n",
      "8   TfidfVectorizer             Stemming           MultinomialNB  0.878800   \n",
      "\n",
      "    Training Time (s)  \n",
      "4           35.619090  \n",
      "3            1.847332  \n",
      "5            0.019000  \n",
      "1           36.642292  \n",
      "0            1.802860  \n",
      "2            0.019012  \n",
      "10          42.119754  \n",
      "9            1.145295  \n",
      "11           0.021000  \n",
      "7           41.925832  \n",
      "6            1.655458  \n",
      "8            0.019994  \n",
      "\n",
      "Summary of Best Classifiers:\n",
      "                             Vectorizer Stemming/Lemmatizing  Accuracy  \\\n",
      "Model                                                                    \n",
      "DecisionTreeClassifier  TfidfVectorizer             Stemming  0.824233   \n",
      "LogisticRegression      CountVectorizer             Stemming  0.887800   \n",
      "MultinomialNB           CountVectorizer             Stemming  0.879383   \n",
      "\n",
      "                        Training Time (s)  \n",
      "Model                                      \n",
      "DecisionTreeClassifier          41.925832  \n",
      "LogisticRegression               1.802860  \n",
      "MultinomialNB                    0.019012  \n"
     ]
    }
   ],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'DecisionTreeClassifier': DecisionTreeClassifier(),\n",
    "    'MultinomialNB': MultinomialNB()\n",
    "}\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "# Perform classification with different vectorizers and preprocessing techniques\n",
    "for vectorizer, vec_name in zip([count_vectorizer, tfidf_vectorizer], ['CountVectorizer', 'TfidfVectorizer']):\n",
    "    for stemmer_func, stemmer_name in zip([stemmed_tokenizer, lemmatized_tokenizer], ['Stemming', 'Lemmatizing']):\n",
    "        print(f'\\nUsing {vec_name} with {stemmer_name}:\\n')\n",
    "        \n",
    "        # Transform text data\n",
    "        X_train_vec = vectorizer.fit_transform(X_train.apply(lambda x: ' '.join(stemmer_func(x))))\n",
    "        X_test_vec = vectorizer.transform(X_test.apply(lambda x: ' '.join(stemmer_func(x))))\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f'\\nTraining {model_name}:\\n')\n",
    "            \n",
    "            # Measure training time\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train_vec, y_train)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred = model.predict(X_test_vec)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            training_time = end_time - start_time\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'Vectorizer': vec_name,\n",
    "                'Stemming/Lemmatizing': stemmer_name,\n",
    "                'Model': model_name,\n",
    "                'Accuracy': accuracy,\n",
    "                'Training Time (s)': training_time,\n",
    "                'Classification Report': report\n",
    "            })\n",
    "            \n",
    "            print(f'Accuracy: {accuracy}')\n",
    "            print(f'Training Time: {training_time:.2f} seconds')\n",
    "            print(f'Classification Report:\\n{classification_report(y_test, y_pred)}')\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the best performing model and parameters\n",
    "best_results = results_df.loc[results_df.groupby(['Vectorizer', 'Stemming/Lemmatizing', 'Model'])['Accuracy'].idxmax()]\n",
    "\n",
    "# Display the best results\n",
    "print(\"\\nBest Results by Vectorizer, Stemming/Lemmatizing, and Model:\")\n",
    "print(best_results[['Vectorizer', 'Stemming/Lemmatizing', 'Model', 'Accuracy', 'Training Time (s)']])\n",
    "\n",
    "# Create a summary table of the best classifiers\n",
    "summary_table = best_results.groupby('Model').apply(lambda x: x.loc[x['Accuracy'].idxmax()])\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\nSummary of Best Classifiers:\")\n",
    "print(summary_table[['Vectorizer', 'Stemming/Lemmatizing', 'Accuracy', 'Training Time (s)']])\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "#### Summary of Best Classifiers:\n",
    "                             Vectorizer Stemming/Lemmatizing  Accuracy  \\\n",
    "Model                                                                    \n",
    "DecisionTreeClassifier  TfidfVectorizer             Stemming  0.824233   \n",
    "LogisticRegression      CountVectorizer             Stemming  0.887800   \n",
    "MultinomialNB           CountVectorizer             Stemming  0.879383   \n",
    "\n",
    "                        Training Time (s)  \n",
    "Model                                      \n",
    "DecisionTreeClassifier          41.925832  \n",
    "LogisticRegression               1.802860  \n",
    "MultinomialNB                    0.019012  \n",
    "\n",
    "### Analysis\n",
    "\n",
    "#### Accuracy\n",
    "Logistic Regression with CountVectorizer and Stemming achieved the highest accuracy of 0.887800. This indicates that Logistic Regression performed the best in classifying the text data, with a significant margin over the other classifiers.\n",
    "\n",
    "MultinomialNB with CountVectorizer and Stemming had an accuracy of 0.879383, which is also high but slightly lower than Logistic Regression.\n",
    "\n",
    "DecisionTreeClassifier with TfidfVectorizer and Stemming achieved an accuracy of 0.824233. While it still performed well, it was less accurate compared to the other models.\n",
    "\n",
    "#### Training Time\n",
    "MultinomialNB had the shortest training time of 0.019012 seconds, making it the fastest classifier. This is expected as it was mentioned in the course earlier MultinomialNB is generally efficient with large datasets, particularly for text classification.\n",
    "\n",
    "Logistic Regression took 1.802860 seconds to train, which is relatively quick but significantly longer than MultinomialNB.\n",
    "\n",
    "DecisionTreeClassifier took 41.925832 seconds, making it the slowest among the three. This is expected since decision trees are known for their complexity, especially with large feature sets and deep trees.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "If you prioritize accuracy and can afford slightly longer training times, Logistic Regression is the best choice. If you need the fastest training time and can accept a minor reduction in accuracy, MultinomialNB is preferable. DecisionTreeClassifier provides a decent accuracy but at a much higher training cost, making it less suitable if speed is a concern.\n",
    "\n",
    "In summary, I would recommend Logistic Regression for best accuracy, while MultinomialNBÂ  for best speed in this particular case study. The choice of vectorizer (CountVectorizer vs. TfidfVectorizer) and text preprocessing technique (Stemming vs. Lemmatizing) showed minimal impact on the overall best performing model, but these aspects could still be worth exploring further depending on specific requirements and constraints of the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
